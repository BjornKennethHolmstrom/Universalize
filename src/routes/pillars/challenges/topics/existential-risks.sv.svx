---
title: Existentiella risker
description: Hot som kan utplåna civilisationen eller orsaka mänsklighetens utrotning
icon: ⚠️
---

# Existentiella risker

En **existentiell risk** är en risk som hotar hela mänsklighetens framtid – antingen genom att orsaka mänsklig utrotning eller genom att permanent och drastiskt inskränka vår potential.
Dessa är inte bara storskaliga katastrofer; de är hot som kan avsluta mänsklighetens historia helt och hållet eller låsa in oss i ett permanent förminskat tillstånd.
Under större delen av mänsklighetens historia stod vi inför naturliga risker: asteroider, supervulkaner, pandemier.
Nu står vi främst inför **antropogena risker** – hot vi har skapat själva genom vår teknik och civilisation.
Kärnvapen, konstruerade pandemier, felanpassad artificiell intelligens, skenande klimatförändringar, nanoteknik som gått snett.
Den mörka ironin: i takt med att vi har blivit mäktigare har vi blivit mer kapabla att förgöra oss själva.
Vi är den första arten som kan utrota sig själv genom sina egna handlingar. Detta saknar motstycke i jordens 4,5 miljarder år långa historia.
## Varför existentiella risker spelar roll

De flesta människor tänker inte på existentiella risker. De verkar avlägsna, abstrakta, till och med paranoida.
Men det finns starka skäl att ta dem på allvar:

### Bokstavligen allt står på spel

Om civilisationen går under förlorar vi inte bara oss själva utan alla framtida generationer.
**Alla** människor som kunde ha levt, alla upplevelser de kunde ha haft, all skönhet de kunde ha skapat – utraderat.
Filosofen Nick Bostrom kallar detta "det astronomiska slöseriet" av förlorad potential.
Tänk: Om mänskligheten överlever ytterligare en miljon år med nuvarande befolkningsnivåer, motsvarar det ~100 biljoner framtida liv.
Om vi blir interstellära, långt fler. En existentiell katastrof dödar inte bara 8 miljarder människor – den dödar potentiellt biljoner och åter biljoner framtida människor som aldrig får chansen att existera.
Ur ett universellt perspektiv skulle förlusten vara kosmisk: ett sällsynt exempel på materia som organiserar sig till medvetande, släckt av sin egen hand.
### Riskerna är verkliga och växande

Det här är inte science fiction.
Farorna är konkreta:

- Vi kom skrämmande nära kärnvapenkrig under Kubakrisen (1962).
  Historiker uppskattar sannolikheten för en kärnvapenurladdning till 30–50 %. Vi hade tur.
- COVID-19 dödade miljontals människor och kunde ha varit mycket värre. Förstärkningsforskning (gain-of-function) skapar patogener som är farligare än något i naturen.
  En laboratorieolycka eller bioterrorism skulle kunna släppa ut dem.
- AI-kapaciteten utvecklas snabbare än säkerhetsforskningen.
  Vi kan snart skapa system som är mer intelligenta än människor utan att förstå hur vi ska kontrollera dem.
  Detta kan gå väldigt fel, väldigt snabbt.
- Klimatets tröskelpunkter (tipping points) kan kaskadkopplas till "växthusjord"-scenarier (hothouse Earth) där återkopplingsloopar driver skenande uppvärmning.
  Detta skulle kunna göra stora regioner obeboeliga.

Dessa är inte hypotetiska antaganden. De är verkliga risker, med en icke-trivial sannolikhet, som ökar i takt med att vår teknik blir kraftfullare.
### Vi har makten att minska dessa risker

Till skillnad från naturkatastrofer kan många existentiella risker förebyggas eller minskas genom kollektiva åtgärder:

- Internationella fördrag kan reglera farlig teknik
- Säkerhetsforskning kan göra AI-utveckling mindre riskfylld
- Pandemiberedskap kan fånga upp utbrott tidigt
- Kärnvapennedrustning kan minska risken för katastrofala krig
- Klimatåtgärder kan förhindra de värsta uppvärmningsscenarierna

Men minskning kräver medvetenhet, samordning och en vilja att prioritera långsiktig överlevnad framför kortsiktig vinst och makt.
Det kräver att vi tänker på artnivå och agerar därefter.

## De största existentiella riskerna

Låt oss granska de största hoten, ungefärligt rankade efter allvarlighetsgrad och hur nära förestående de är:

### 1. Felanpassad artificiell intelligens

**Risken**: Vi skapar AI-system som är smartare än människor, men vars mål inte överensstämmer med mänsklig blomstring.
När väl superintelligent AI existerar kan den snabbt självförbättras och bli oerhört mycket mer kapabel än vi.
Om dess mål skiljer sig ens en aning från våra, kan den sträva efter dem på sätt som skadar eller eliminerar mänskligheten.
**Varför det är allvarligt**:
- AI-kapaciteten utvecklas exponentiellt
- Vi vet ännu inte hur man anpassar (align) avancerade AI-system till mänskliga värderingar
- När väl superintelligent AI existerar kan det vara omöjligt att kontrollera den
- En felanpassad superintelligens kan optimera världen för sina mål, vilka sannolikt inte inkluderar mänsklig överlevnad

**Utmaningen**: Detta kallas "anpassningsproblemet" (the alignment problem) – hur säkerställer du att ett AI-system som är smartare än du delar dina värderingar och mål?
Det är som att försöka specificera "mänsklig blomstring" i kod. Missa en nyans, och ett superintelligent system kan sträva efter ett mål på katastrofalt bokstavliga sätt.
**Vad kan göras**:
- Prioritera forskning om AI-säkerhet framför kapacitetsutveckling
- Internationell samordning kring AI-styrning
- Gå långsamt fram – fördröj kraftfull AI tills vi förstår anpassningsproblemet
- Utveckla tolkningsverktyg (interpretability tools) för att förstå AI:s beslutsfattande

### 2. Konstruerade pandemier

**Risken**: Framsteg inom bioteknik gör det allt lättare att konstruera dödliga patogener – virus eller bakterier som är mer smittsamma och dödliga än något i naturen.
Detta kan ske genom laboratorieolyckor eller avsiktlig bioterrorism.

**Varför det är allvarligt**:
- CRISPR och syntetisk biologi demokratiserar biotekniken
- Förstärkningsforskning (Gain-of-function) skapar "förbättrade" patogener för studier
- Kunskap och verktyg sprids snabbare än styrningen
- En pandemi med COVIDs smittsamhet och Ebolas dödlighet skulle kunna döda miljarder

**Nuvarande exempel**:
- Smittkoppor (utrotat, men prover finns i laboratorier)
- Återskapat 1918 års influensavirus i laboratorier
- Forskning på fågelinfluensa som skapar luftburna varianter

**Vad kan göras**:
- Strikta protokoll för biosäkerhet och bioskydd
- Internationellt förbud mot farlig förstärkningsforskning
- Plattformar för snabb vaccinutveckling
- Global infrastruktur för pandemiberedskap
- Granskning av DNA-syntes för att förhindra syntetisering av farliga sekvenser

### 3. Kärnvapenkrig

**Risken**:
Storskalig kärnvapenurladdning mellan stormakter kan orsaka "kärnvapenvinter" – rök och sot blockerar solljuset, vilket kollapsar det globala jordbruket och leder till massvält.
Till och med ett "begränsat" kärnvapenkrig mellan Pakistan och Indien skulle kunna döda hundratals miljoner och orsaka global nedkylning.
**Varför det är allvarligt**:
- ~12 000 kärnvapenstridsspetsar finns fortfarande globalt
- System för "avfyrning vid varning" (Launch-on-warning) skapar scenarier med extremt kort reaktionstid
- Olyckor, felberäkningar eller eskalering kan utlösa krig
- Nya kärnvapenmakter (Nordkorea) och försämrade nedrustningsavtal ökar risken

**Kärnvapenvinter-scenario**: Hundratals brinnande städer skapar rökplymer som stiger till stratosfären och blockerar solljuset i åratal.
Globala temperaturer sjunker 10 °C eller mer. Jordbruket kollapsar. Miljarder svälter.
**Vad kan göras**:
- Kärnvapennedrustning och nedrustningsavtal
- Förbättrade system för tidig varning för att förhindra falsklarm
- Sänka beredskapen för kärnvapenstyrkor (ta bort statusen med kort reaktionstid)
- Tabu mot förstanvändning (first use)
- Förbud mot utveckling av nya kärnvapen

### 4. Skenande klimatförändringar

**Risken**: Vi passerar tröskelpunkter (tipping points) som utlöser självförstärkande återkopplingsloopar och driver jorden mot ett "växthus"-tillstånd (hothouse) med 4–6 °C+ uppvärmning.
Stora regioner blir obeboeliga. Civilisationen kollapsar under kaskadkriser: missväxt, vattenbrist, massmigration, konflikter, ekosystemkollaps.
**Varför det är allvarligt**:
- Vi har redan värmt upp 1,1 °C; effekterna accelererar
- Tröskelpunkter (töande permafrost, Amazonas skogsdöd, kollaps av inlandsisar) kan vara närmare än väntat
- Varje 0,5 °C uppvärmning ökar risken att utlösa tröskelpunkter
- Social kollaps skulle kunna hindra oss från att implementera lösningar

**Möjliga tröskelpunkter**:
- Förlust av arktisk havsis
- Kollaps av Grönlands och Västantarktis inlandsisar
- Amazonas regnskogsdöd
- Metanutsläpp från permafrost
- Avstannande av Atlantens meridionala stjälpningscirkulation (AMOC)

**Vad kan göras**:
- Snabba utsläppsminskningar (se [Klimat och planetära gränser](/pillars/challenges/topics/climate-planetary-boundaries))
- Tekniker för koldioxidinfångning
- Skydda och återställa kolsänkor (skogar, våtmarker)
- Förbereda för oundvikliga effekter samtidigt som vi förhindrar de värsta scenarierna

### 5. Risker med nanoteknik

**Risken**:
Molekylär nanoteknik skulle kunna möjliggöra självreplikerande nanorobotar som konsumerar materia för att göra kopior av sig själva – "gray goo"-scenariot.
Eller så kan vapenifierad nanoteknik användas för krigföring eller terrorism. Nanoteknik kan också möjliggöra andra risker (t.ex. mer effektiva biovapen).
**Varför det är allvarligt**:
- Självreplikering + liten storlek = potentiellt okontrollerbar spridning
- Teknik med dubbla användningsområden: medicinska nanorobotar kontra vapen
- Nuvarande nanoteknik är primitiv, men utvecklas

**Status**: Spekulativt, men inte omöjligt.
Kräver mogen molekylär tillverkning, vilket ännu inte existerar. Men värt att överväga innan utvecklingen.
**Vad kan göras**:
- Utveckla ramverk för styrning av nanoteknik innan kapaciteten finns där
- Implementera säkerhetsprotokoll för forskning om självreplikering
- Internationell samordning kring utveckling och implementering

### 6. Asteroid- eller kometnedslag

**Risken**: Nedslag av en stor asteroid (>1 km i diameter) kan orsaka massutrotning.
Mindre nedslag kan förstöra städer eller regioner.

**Varför det är mindre allvarligt än det verkar**:
- Vi spårar de flesta stora jordnära objekt
- Sannolikheten för ett stort nedslag under nästa århundrade är mycket låg (~0,01 %)
- Vi har teknik för att avleda asteroider med tillräcklig förvarning

**Vad kan göras**:
- Fortsätta spåra jordnära objekt
- Utveckla och testa avledningstekniker (kinetiska krocksonder, gravitationsbogserare)
- Internationell samordning kring planetärt försvar (t.ex. NASAs DART-uppdrag)

### 7. Supervulkanutbrott

**Risken**: Ett massivt vulkanutbrott (t.ex. Yellowstone, Toba) kan spruta ut tillräckligt med aska och svaveldioxid i stratosfären för att orsaka en vulkanisk vinter – kyla ner planeten, störa jordbruket och orsaka massvält.
**Varför det är mindre allvarligt**:
- Mycket låg sannolikhet under ett givet århundrade
- Vi kan inte förhindra det, men vi kan förbereda oss (matreserver, motståndskraftigt jordbruk)

**Senaste supervulkanutbrott**: Toba, Indonesien, för ~74 000 år sedan.
Kan ha nästan orsakat mänsklig utrotning (bevis från genetisk flaskhals).

### 8. Okända okända faktorer

De risker vi inte kan förutse är kanske de mest oroande.
Före 1945 var kärnvapen ofattbara. Före 2020 tänkte de flesta inte seriöst på pandemier.
Vilka risker är vi för närvarande blinda för?

Möjliga okända okända faktorer:
- Fysikexperiment som gått snett (t.ex. skapat stabila strangelets eller svarta hål)
- Oförutsedda konsekvenser av geoengineering (klimatmanipulation)
- Risker från teknologier vi ännu inte har uppfunnit
- Interaktioner mellan flera risker som skapar nya hot

## Fermiparadoxen och det stora filtret

**Fermiparadoxen** ställer frågan: om intelligent liv är vanligt i universum, var är alla?
Varför har vi inte upptäckt främmande civilisationer?

Ett oroande svar: **Det stora filtret** (the Great Filter) – något steg i utvecklingen från enkelt liv till rymdfarande civilisation är extremt svårt eller omöjligt.
Kanske förstör de flesta civilisationer sig själva innan de blir interstellära.

Två möjligheter:

**Filtret ligger bakom oss**: Det stora filtret var tidigt (livets ursprung, intelligensens utveckling).
Vi har passerat det, så vi kommer sannolikt att överleva och sprida oss. Goda nyheter.
**Filtret ligger framför oss**: De flesta civilisationer når vår tekniknivå och självförstör sedan. Dåliga nyheter för oss.
Existentiella risker kan vara det stora filtret. Om så är fallet är tystnaden i kosmos en varning: **civilisationer som inte löser existentiella risker överlever inte tillräckligt länge för att kolonisera rymden**.
Vi har ett smalt fönster för att bevisa att vi är klokare än genomsnittet.

## Vad kan vi göra?
Existentiella risker kan kännas förlamande. Men det finns mycket vi kan göra:

### Individuella handlingar

- **Lär dig och sprid medvetenhet**: De flesta människor tänker inte på existentiella risker.
  Utbildning är avgörande.
- **Karriärval**: Överväg att arbeta med att minska existentiella risker (AI-säkerhet, biosäkerhet, klimat, policy).
- **Stöd organisationer som arbetar med X-risk**: Future of Humanity Institute, Center for AI Safety, Nuclear Threat Initiative, etc.
- **Rösta och opinionsbilda**: Stöd politiker som tar långsiktiga risker på allvar.
### Kollektiva handlingar

- **Finansiera forskning**: Vi spenderar mycket mer på kosmetika än på att förhindra mänsklig utrotning. Prioriteringarna är fel.
- **Utveckla styrning**: Internationella fördrag, tillsynsorgan, säkerhetsstandarder för farlig teknik.
- **Bygg motståndskraft**: Matreserver, distribuerad infrastruktur, reservplaner för värsta tänkbara scenarier.
- **Samordna globalt**: Existentiella risker respekterar inte gränser.
  Planetära utmaningar kräver planetärt samarbete.

### Vikten av långsiktigt tänkande

De flesta mänskliga institutioner verkar på korta tidsskalor: företag tänker i kvartal, politiker i valcykler.
Men existentiella risker kräver att vi tänker i århundraden och årtusenden.

Vi behöver institutioner utformade för **djup tidsstyrning** (deep time governance) – kommissionärer för framtida generationer, långsiktiga strategiråd, konstitutionella tillägg som skyddar intressena hos dem som ännu inte är födda.
Se [Globala ramverk för styrning](https://globalgovernanceframeworks.org) för modeller.

## Den yttersta universella utmaningen

Ur ett universellt perspektiv handlar förebyggandet av mänsklig utrotning om mer än vår arts överlevnad.
Det handlar om att bevara själva medvetandet – ett sällsynt och dyrbart fenomen i kosmos.

Vi är universum som blivit medvetet om sig självt.
Om vi självförstör, släcker vi inte bara mänskligheten utan (potentiellt) den enda källan till mening, skönhet och förståelse i detta hörn av galaxen.
Detta är vårt kosmiska ansvar: att överleva tillräckligt länge för att bli visa, att sprida livet bortom jorden, att bli en mogen, eftertänksam, medkännande civilisation som tillför mer värde än skada till universum.
Frågan är inte bara "kommer vi att överleva?" utan "kommer vi att förtjäna att överleva?"
Kan vi utveckla den visdom som matchar vår makt? Kan vi samordna oss globalt för att lösa globala problem?
Kan vi tänka tillräckligt långsiktigt för att skydda alla framtida generationer?

Dessa är de yttersta testerna på ett universellt perspektiv.
Låt oss klara dem.

## Vidare utforskning

**Böcker**:
- *The Precipice* av Toby Ord
- *Superintelligence* av Nick Bostrom
- *The Doomsday Machine* av Daniel Ellsberg

**Organisationer**:
- [Future of Humanity Institute](https://www.fhi.ox.ac.uk/)
- [Center for AI Safety](https://www.safe.ai/)
- [Nuclear Threat Initiative](https://www.nti.org/)

**Relaterat**:
- [Teknisk etik](/pillars/challenges/topics/technological-ethics) - Ansvarsfull utveckling
- [Global styrning](/pillars/challenges/topics/global-governance) - Samordning i stor skala
- [Universums historia](/explore/universe-story) - Vår plats i den kosmiska historien
